## üìñ –ë—ã—Å—Ç—Ä–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è / Quick Navigation

üá∑üá∫ [–†—É—Å—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](#-—Ä—É—Å—Å–∫–∞—è-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è) | üá¨üáß [English Documentation](#-english-documentation)

---

# üá∑üá∫ –†—É—Å—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

# üîÑ vLLM/LLM Model BNB4 Converter

**–£–¥–æ–±–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä AI-–º–æ–¥–µ–ª–µ–π –≤ 4-–±–∏—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (bnb4) —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º**

–ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ –∫–≤–∞–Ω—Ç–æ–≤–∞—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ AI-–º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞—Ö. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Vision-Language –º–æ–¥–µ–ª–∏ –∏ –æ–±—ã—á–Ω—ã–µ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ç–∏–ø–∞ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.

## ‚ú® –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **üñºÔ∏è –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π:**
  - Vision-Language –º–æ–¥–µ–ª–∏ (NuMarkdown, Qwen2.5-VL, MiniCPM-V –∏ –¥—Ä.)
  - –ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ (Hunyuan-MT, NLLB, M2M100)
  - –ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Qwen, Mistral, Llama –∏ –¥—Ä.)

- **‚öôÔ∏è –ì–∏–±–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏:**
  - –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (GPU/CPU/Auto)
  - –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ—Ç 512 –¥–æ 262k —Ç–æ–∫–µ–Ω–æ–≤)
  - –¢–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (NF4/FP4)
  - –§–æ—Ä–º–∞—Ç SafeTensors

- **üéÆ –ü—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:**
  - –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
  - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  - –ü–æ–¥—Ä–æ–±–Ω—ã–π –∂—É—Ä–Ω–∞–ª –æ–ø–µ—Ä–∞—Ü–∏–π —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
  - –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
  - –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫

- **üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞:**
  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ CUDA
  - –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –≤–µ—Ä—Å–∏–∏ PyTorch
  - –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ GPU, —Ç–∞–∫ –∏ CPU-—Ä–µ–∂–∏–º–æ–≤

<img width="1102" height="832" alt="image" src="https://github.com/user-attachments/assets/4a86929a-40e3-48d5-8f2e-3343325ab18c" />


## üìã –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- **–û–°:** Windows 10/11
- **Python:** 3.8 –∏–ª–∏ –Ω–æ–≤–µ–µ
- **–û–ó–£:** 8 GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16+ GB)
- **–°–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ:** 50+ GB –¥–ª—è –º–æ–¥–µ–ª–µ–π

### –î–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ GPU:
- **–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞:** NVIDIA —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA
- **–í–∏–¥–µ–æ–ø–∞–º—è—Ç—å:** 8+ GB
- **–î—Ä–∞–π–≤–µ—Ä—ã:** –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫

1. **–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è:**
```bash
git clone https://github.com/username/BNB4-Model-converter.git
cd BNB4-Model-converter
```

2. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:**
```bash
install.bat
```
–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç –≤–µ—Ä—Å–∏—é Python
- –°–æ–∑–¥–∞—Å—Ç –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
- –û–ø—Ä–µ–¥–µ–ª–∏—Ç –≤–µ—Ä—Å–∏—é CUDA –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π PyTorch
- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏

3. **–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã:**
```bash
run.bat
```

## üéØ –ü—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç—ã

### –®–∞–≥ 1: –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
- –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ URL –≤—Ä—É—á–Ω—É—é
- –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∫–∞–∂–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ–π

### –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** Auto (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è), GPU –∏–ª–∏ CPU
- **–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:** –í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≤–∞—à–∏—Ö –∑–∞–¥–∞—á  
- **–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ:** NF4 (bnb4) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è 4-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
- **–ü–∞–ø–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è:** –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

### –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
- –ù–∞–∂–º–∏—Ç–µ "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é"
- –°–ª–µ–¥–∏—Ç–µ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- –ü–æ–ª—É—á–∏—Ç–µ –≥–æ—Ç–æ–≤—É—é –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ

## üí° –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è

- **–≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏:** –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –≤ ~4 —Ä–∞–∑–∞
- **–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å:** –ó–∞–ø—É—Å–∫ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö GPU
- **–°–∫–æ—Ä–æ—Å—Ç—å:** –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ –∫–∞—á–µ—Å—Ç–≤–∞
- **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:** –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π transformers

## üîß –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫** - –≤–∞—à–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏
- **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ** - –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫** - –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö –∏ —Å–ø–æ—Å–æ–±–∞—Ö –∏—Ö —Ä–µ—à–µ–Ω–∏—è
- **–ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞** - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

***

‚≠ê **–ü–æ—Å—Ç–∞–≤—å –∑–≤–µ–∑–¥—É, –µ—Å–ª–∏ –±—ã–ª–æ –ø–æ–ª–µ–∑–Ω–æ!**


# üá¨üáß English Documentation

# üîÑ vLLM/LLM Model BNB4 Converter

**User-friendly AI model converter to 4-bit format (bnb4) with graphical interface**

The application allows easy quantization of modern AI models for efficient use on consumer graphics cards. Supports Vision-Language models and regular LLMs with automatic type detection and optimal settings.

## ‚ú® Key Features

- **üñºÔ∏è Support for various model types:**
  - Vision-Language models (NuMarkdown, Qwen2.5-VL, MiniCPM-V, etc.)
  - Machine translation models (Hunyuan-MT, NLLB, M2M100)
  - Large Language Models (Qwen, Mistral, Llama, etc.)

- **‚öôÔ∏è Flexible conversion settings:**
  - Processing device selection (GPU/CPU/Auto)
  - Context length configuration (from 512 to 262k tokens)
  - Quantization type (NF4/FP4)
  - SafeTensors format

- **üéÆ Simple graphical interface:**
  - Intuitive user interface
  - Grouped list of popular models
  - Detailed operation log with timestamps
  - Real-time progress indicator
  - Auto-save user settings

- **üöÄ Automatic installation:**
  - Automatic CUDA version detection
  - Compatible PyTorch installation
  - Dependency conflict resolution
  - Support for both GPU and CPU modes

<img width="1102" height="832" alt="image" src="https://github.com/user-attachments/assets/e170c302-8620-4b86-a2ef-211ad5f992cb" />


## üìã System Requirements

### Minimum Requirements:
- **OS:** Windows 10/11
- **Python:** 3.8 or newer
- **RAM:** 8 GB (16+ GB recommended)
- **Free space:** 50+ GB for models

### For GPU operation:
- **Graphics card:** NVIDIA with CUDA support
- **VRAM:** 8+ GB
- **Drivers:** Latest NVIDIA drivers

## üõ†Ô∏è Installation and Launch

1. **Clone repository:**
```bash
git clone https://github.com/username/BNB4-Model-converter.git
cd BNB4-Model-converter
```

2. **Automatic dependency installation:**
```bash
install.bat
```
The script automatically:
- Checks Python version
- Creates virtual environment
- Detects CUDA version and installs compatible PyTorch
- Installs all necessary libraries

3. **Launch application:**
```bash
run.bat
```

## üéØ Workflow

### Step 1: Model Selection
- Choose a model from the preset list or enter URL manually
- The program automatically detects model type and displays information

### Step 2: Parameter Configuration
- **Device:** Auto (recommended), GPU, or CPU
- **Context length:** Choose appropriate value for your tasks
- **Quantization:** NF4 (bnb4) - standard for 4-bit quantization
- **Save folder:** Specify path for model storage

### Step 3: Start Conversion
- Click "üöÄ Start Conversion"
- Monitor progress in real-time
- Get ready quantized model in specified folder

## üí° Quantization Benefits

- **Memory savings:** Model size reduction by ~4x
- **Accessibility:** Running large models on consumer GPUs
- **Speed:** Inference acceleration with minimal quality loss
- **Compatibility:** Full compatibility with transformers library

## üîß Additional Features

- **Auto-save settings** - your preferences are saved between sessions
- **Logging** - complete information about the conversion process
- **Error handling** - clear error messages and solution guidance
- **Flexible configuration** - ability to add custom models

If you encounter any issues or have questions, create an Issue in this repository.

***

‚≠ê **Star this repository if it was helpful!**
